{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q streamlit"
      ],
      "metadata": {
        "id": "PJiOJhnMMuIb"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import streamlit as st\n",
        "import torch\n",
        "import time\n",
        "import re\n",
        "import numpy as np\n",
        "import tempfile\n",
        "import soundfile as sf\n",
        "\n",
        "from unsloth import FastModel\n",
        "from transformers import TrainingArguments\n",
        "from trl import SFTTrainer\n",
        "import torchaudio.transforms as T\n",
        "import sys\n",
        "sys.path.append('--app--')\n",
        "from sparktts.models.audio_tokenizer import BiCodecTokenizer\n",
        "from sparktts.utils.audio import audio_volume_normalize\n",
        "\n",
        "st.set_page_config(page_title=\"Voice Assistant\", page_icon=\"ðŸŽ¤\", layout=\"centered\")\n",
        "\n",
        "st.markdown(\"\"\"\n",
        "    <style>\n",
        "    body {\n",
        "        background-color: #0b0f19;\n",
        "        color: white;\n",
        "    }\n",
        "    .stApp {\n",
        "        background: linear-gradient(180deg, #0b0f19 0%, #111927 100%);\n",
        "    }\n",
        "    .main-box {\n",
        "        background-color: #1c2230;\n",
        "        padding: 2rem;\n",
        "        border-radius: 12px;\n",
        "        text-align: center;\n",
        "    }\n",
        "    </style>\n",
        "\"\"\", unsafe_allow_html=True)\n",
        "\n",
        "st.markdown(\"<h2 style='text-align: center; color: #c084fc;'>ðŸŽ¤ Knowlithic TTS Demo</h2>\", unsafe_allow_html=True)\n",
        "st.markdown(\"<p style='text-align: center; color: gray;'>Convert your text into speech using Knowlithic</p>\", unsafe_allow_html=True)\n",
        "st.markdown('<div class=\"main-box\">', unsafe_allow_html=True)\n",
        "\n",
        "@st.cache_resource\n",
        "def load_tts_model():\n",
        "    max_seq_length = 2048\n",
        "    audio_tokenizer = BiCodecTokenizer(\"mobeen0/tokenizer\", \"cuda\")\n",
        "\n",
        "    model, tokenizer = FastModel.from_pretrained(\n",
        "    model_name = f\"mobeen0/knowlithic-0.4\",\n",
        "    max_seq_length = 2048,\n",
        "    dtype = torch.float32,\n",
        "    full_finetuning = True,\n",
        "    load_in_4bit = False,\n",
        "    token = \"--replace-with-your-token--\",\n",
        "    )\n",
        "    FastModel.for_inference(model)\n",
        "    return model, tokenizer, audio_tokenizer\n",
        "\n",
        "model, tokenizer, audio_tokenizer = load_tts_model()\n",
        "\n",
        "@torch.inference_mode()\n",
        "def generate_speech_from_text(\n",
        "    text: str,\n",
        "    model,\n",
        "    tokenizer,\n",
        "    audio_tokenizer,\n",
        "    temperature: float = 0.8,\n",
        "    top_k: int = 50,\n",
        "    top_p: float = 1.0,\n",
        "    max_new_audio_tokens: int = 2048,\n",
        "    device: torch.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        ") -> np.ndarray:\n",
        "    prompt = \"\".join([\n",
        "        \"<|task_tts|>\",\n",
        "        \"<|start_content|>\",\n",
        "        text,\n",
        "        \"<|end_content|>\",\n",
        "        \"<|start_global_token|>\"\n",
        "    ])\n",
        "\n",
        "    model_inputs = tokenizer([prompt], return_tensors=\"pt\").to(device)\n",
        "\n",
        "    generated_ids = model.generate(\n",
        "        **model_inputs,\n",
        "        max_new_tokens=max_new_audio_tokens,\n",
        "        do_sample=True,\n",
        "        temperature=temperature,\n",
        "        top_k=top_k,\n",
        "        top_p=top_p,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        pad_token_id=tokenizer.pad_token_id\n",
        "    )\n",
        "\n",
        "    generated_ids_trimmed = generated_ids[:, model_inputs.input_ids.shape[1]:]\n",
        "    predicts_text = tokenizer.batch_decode(generated_ids_trimmed, skip_special_tokens=False)[0]\n",
        "\n",
        "    semantic_matches = re.findall(r\"<\\|bicodec_semantic_(\\d+)\\|>\", predicts_text)\n",
        "    if not semantic_matches:\n",
        "        return np.array([], dtype=np.float32)\n",
        "\n",
        "    pred_semantic_ids = torch.tensor([int(token) for token in semantic_matches]).long().unsqueeze(0)\n",
        "\n",
        "    global_matches = re.findall(r\"<\\|bicodec_global_(\\d+)\\|>\", predicts_text)\n",
        "    pred_global_ids = (\n",
        "        torch.tensor([int(token) for token in global_matches]).long().unsqueeze(0)\n",
        "        if global_matches else torch.zeros((1, 1), dtype=torch.long)\n",
        "    ).unsqueeze(0)\n",
        "\n",
        "    audio_tokenizer.device = device\n",
        "    audio_tokenizer.model.to(device)\n",
        "    wav_np = audio_tokenizer.detokenize(\n",
        "        pred_global_ids.to(device).squeeze(0),\n",
        "        pred_semantic_ids.to(device)\n",
        "    )\n",
        "\n",
        "    return wav_np\n",
        "\n",
        "\n",
        "st.markdown(\"## ðŸ”Š Text-to-Speech\")\n",
        "\n",
        "user_text = st.text_input(\"Type something...\", \"\")\n",
        "\n",
        "if st.button(\"Generate Audio\"):\n",
        "    if not user_text.strip():\n",
        "        st.warning(\"Please enter text to synthesize.\")\n",
        "    else:\n",
        "        with st.spinner(\"Synthesizing...\"):\n",
        "            try:\n",
        "                generated_waveform = generate_speech_from_text(\n",
        "                    user_text, model, tokenizer, audio_tokenizer\n",
        "                )\n",
        "\n",
        "                if generated_waveform.size == 0:\n",
        "                    st.error(\"Failed to generate audio.\")\n",
        "                else:\n",
        "                    sample_rate = audio_tokenizer.config.get(\"sample_rate\", 16000)\n",
        "\n",
        "                    with tempfile.NamedTemporaryFile(suffix=\".wav\", delete=False) as tmpfile:\n",
        "                        sf.write(tmpfile.name, generated_waveform, sample_rate)\n",
        "                        st.success(\"Audio generated!\")\n",
        "                        st.audio(tmpfile.name, format=\"audio/wav\")\n",
        "            except Exception as e:\n",
        "                st.error(f\"An error occurred: {e}\")\n",
        "\n",
        "st.markdown('</div>', unsafe_allow_html=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bDI06IldM0Uk",
        "outputId": "9781b247-3ced-4e7a-d2dd-3eb6b22f429b"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-05-20 15:22:03.382 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-05-20 15:22:03.383 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-05-20 15:22:03.384 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-05-20 15:22:03.385 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-05-20 15:22:03.385 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-05-20 15:22:03.386 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-05-20 15:22:03.386 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-05-20 15:22:03.387 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-05-20 15:22:03.387 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-05-20 15:22:03.388 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-05-20 15:22:03.389 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-05-20 15:22:03.389 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-05-20 15:22:03.891 Thread 'Thread-17': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-05-20 15:22:03.892 Thread 'Thread-17': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing tensor: mel_transformer.spectrogram.window\n",
            "Missing tensor: mel_transformer.mel_scale.fb\n",
            "==((====))==  Unsloth 2025.5.6: Fast Qwen2 patching. Transformers: 4.51.3.\n",
            "   \\\\   /|    NVIDIA L4. Num GPUs = 1. Max memory: 22.161 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.9. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
            "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
            "Unsloth: Float16 full finetuning uses more memory since we upcast weights to float32.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-05-20 15:22:18.341 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-05-20 15:22:18.343 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-05-20 15:22:18.345 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-05-20 15:22:18.346 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-05-20 15:22:18.347 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-05-20 15:22:18.347 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-05-20 15:22:18.348 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-05-20 15:22:18.349 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-05-20 15:22:18.349 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-05-20 15:22:18.350 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-05-20 15:22:18.351 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-05-20 15:22:18.352 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-05-20 15:22:18.352 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-05-20 15:22:18.353 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-05-20 15:22:18.353 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-05-20 15:22:18.354 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-05-20 15:22:18.355 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DeltaGenerator()"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!npm install localtunnel\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EeY-YFMkM2lb",
        "outputId": "c02730dd-19cd-4f0b-a680-6d544386b891"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1G\u001b[0Kâ ™\u001b[1G\u001b[0Kâ ¹\u001b[1G\u001b[0Kâ ¸\u001b[1G\u001b[0Kâ ¼\u001b[1G\u001b[0Kâ ´\u001b[1G\u001b[0K\n",
            "up to date, audited 23 packages in 696ms\n",
            "\u001b[1G\u001b[0Kâ ´\u001b[1G\u001b[0K\n",
            "\u001b[1G\u001b[0Kâ ´\u001b[1G\u001b[0K3 packages are looking for funding\n",
            "\u001b[1G\u001b[0Kâ ´\u001b[1G\u001b[0K  run `npm fund` for details\n",
            "\u001b[1G\u001b[0Kâ ´\u001b[1G\u001b[0K\n",
            "2 \u001b[31m\u001b[1mhigh\u001b[22m\u001b[39m severity vulnerabilities\n",
            "\n",
            "To address all issues (including breaking changes), run:\n",
            "  npm audit fix --force\n",
            "\n",
            "Run `npm audit` for details.\n",
            "\u001b[1G\u001b[0Kâ ´\u001b[1G\u001b[0K"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run /content/app.py &>/content/logs.txt & curl ipv4.icanhazip.com"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V5ScQ85ZM4c0",
        "outputId": "3b3c5e8a-d5c3-4709-bf53-f0cae9ddc5ec"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "35.240.201.102\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!npx localtunnel --port 8501"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bwcsvMM0M6Zt",
        "outputId": "05a44725-e8c0-49b8-8955-ea91760cdef9"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1G\u001b[0Kâ ™\u001b[1G\u001b[0Kyour url is: https://kind-cameras-create.loca.lt\n",
            "/content/node_modules/localtunnel/bin/lt.js:81\n",
            "    throw err;\n",
            "    ^\n",
            "\n",
            "Error: connection refused: localtunnel.me:24915 (check your firewall settings)\n",
            "    at Socket.<anonymous> \u001b[90m(/content/\u001b[39mnode_modules/\u001b[4mlocaltunnel\u001b[24m/lib/TunnelCluster.js:52:11\u001b[90m)\u001b[39m\n",
            "\u001b[90m    at Socket.emit (node:events:524:28)\u001b[39m\n",
            "\u001b[90m    at emitErrorNT (node:internal/streams/destroy:169:8)\u001b[39m\n",
            "\u001b[90m    at emitErrorCloseNT (node:internal/streams/destroy:128:3)\u001b[39m\n",
            "\u001b[90m    at process.processTicksAndRejections (node:internal/process/task_queues:82:21)\u001b[39m\n",
            "\n",
            "Node.js v20.19.0\n",
            "\u001b[1G\u001b[0Kâ ™\u001b[1G\u001b[0K"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "dockerImageVersionId": 30919,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}